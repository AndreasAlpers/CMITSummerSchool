{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86f0fbe-1fa6-46d3-8961-d5a9aa32d067",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning: The Cross-Entropy Method\n",
    "\n",
    "## Introduction\n",
    "__Reinforcement Learning (RL)__ sits somewhere between __Supervised__ and __Unsupervised Learning.__\n",
    "\n",
    "* __Supervised Learning__: Training data $(x_i,y_i),$ $i=1,\\dots,n$ with the $x_i$ representing data points and $y_i$ the corresponding label.<br>\n",
    "Examples: Support Vector Machines (SVMs), Bayes Classifiers, Decision Trees, Neural Networks, etc.\n",
    "\n",
    "* __Unsupervised Learning__: One just has data points $x_i,$ $i=1,\\dots,n,$ but not label.<br> \n",
    "Examples: Clustering algorithms, Principal Component Analysis (PCA), \n",
    "\n",
    "__Reinforcement Learning__ follows a different route. RL differs from supervised learning in a way that in supervised learning the training data has the answer key with it so the model is trained with the correct answer itself whereas in RL, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of a training dataset, it is bound to learn from its experience. <br>\n",
    "Examples: \n",
    "* [Game Playing](https://www.deepmind.com/research/highlighted-research/alphago), \n",
    "* [Robotics](https://www.aiperspectives.com/artificial-intelligence-and-robotics), \n",
    "* [Medical Imaging](https://arxiv.org/abs/2103.05115), \n",
    "* [Mathematics (finding constructions/counterexamples to open conjectures)](https://arxiv.org/abs/2104.14516).\n",
    "\n",
    "Reinforcement learning can be viewed as being a technique for solving Markov decision processes.\n",
    "\n",
    "A __Markov decision process__ is a tuple $(\\Omega, A, T, R)$ in which\n",
    "* $\\Omega$ is the set of __states__,\n",
    "* $A$ is a finite set of __actions,__\n",
    "* $T$ is a __transition function__ $T:\\Omega\\times A \\times \\Omega\\to [0,1],$ which, for $T(\\omega,a,\\omega'),$ gives the probability that action $a$ performed in state $\\omega$ will lead to state $\\omega',$\n",
    "* and $R$ is a __reward function__ defined as $R:\\Omega \\times A \\to \\mathbb{R}.$\n",
    "\n",
    "A __policy__ $\\rho$ is a function $\\rho:\\Omega \\to A,$ which tells the agent which action to perform in any given state. \n",
    "\n",
    "The main goal of learning in this context is to learn (i.e., find) a policy that gathers rewards:\n",
    "\n",
    "* DISCOUNTED REWARD: $ \\max\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t r_t]$ with $0\\leq \\gamma \\leq1$ a fixed constant. \n",
    "\n",
    "A lower $\\gamma$ makes \n",
    "rewards from the uncertain far future less important for our agent \n",
    "than the ones in the near future that it can be fairly confident \n",
    "about. It also encourages agents to collect reward closer in time \n",
    "than equivalent rewards that are temporally far away in the future.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: \\Omega \\times A \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\rho^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "\n",
    "### Deep Reinforcement Learning\n",
    "\n",
    "This brings us to Deep Reinforcement Learning, i.e., Reinforcement Learning using deep neural networks. We discuss one particular method, the so-called __Cross-Entropy__ method. This method is, of course, not always the best method, but it has its own strenghts. The most important ones are the following:\n",
    "\n",
    "* __Simplicity__: The cross-entropy method is really simple (<100 lines of code) and intuitive. \n",
    "* __Good convergence__: In environments that don't require complex, multistep policies to be learned and discovered and have short episodes with frequent rewards, cross-entropy usually works very well. \n",
    "\n",
    "We will introduce and discuss this method by discussing the rather popular RL environment FrozenLake. The FrozenLake environment is part of the OpenAI's Gym package, which gives several tools to simulate and compare RL approaches. \n",
    "\n",
    "### FrozenLake\n",
    "\n",
    "You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake (G). The water is mostly frozen (F), but there are a few holes (H) where the ice has melted.\n",
    "\n",
    "![title](Frozen-Lake.png)\n",
    "\n",
    "If you step into one of those holes, you’ll fall into the freezing water. At this time, there’s an international frisbee shortage, so it’s absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won’t always move in the direction you intend.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134b2289-52be-43fa-8243-2287ca98e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e688def8-7621-4337-a3c4-f89e75bbb679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:  Discrete(4)\n",
      "Observation space:  Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, map_name=\"4x4\", render_mode=\"ansi\")\n",
    "print(\"Action space: \", env.action_space)\n",
    "print(\"Observation space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f788b-dace-4d26-9153-c875ce893747",
   "metadata": {},
   "source": [
    "Let's start the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc920ff6-3ddb-445b-8140-1b650aeb5f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d698c-2728-471e-932d-9d7f0add238f",
   "metadata": {},
   "source": [
    "And perform a few steps. The agent has 4 potential actions: 0 (LEFT), 1 (DOWN), 2 (RIGHT), 3 (UP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc3ceef-b607-4665-b87e-d3fd497a6589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Reward = 0.0\n",
      "False\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, episode_is_done, _ , _= env.step(2)\n",
    "print(env.render())\n",
    "print(f'Reward = {reward}')\n",
    "print(episode_is_done)\n",
    "print(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2822342-a5d9-4803-bb39-66cbd707aa1a",
   "metadata": {},
   "source": [
    "In principle, we can work out here the optimal policy. (Idea: Moving backwards. Keyword: Bellman-Ford equation, Dynamic Programming). We want to follow here, however, a deep learning aproach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6902bb-295a-4cc1-a8a9-5b1b17043d88",
   "metadata": {},
   "source": [
    "The following, so-called [wrapper](https://alexandervandekleut.github.io/gym-wrappers/), for OpenAI's gym allows us to add functionality to environments, such as modifying observations and rewards to be fed to our agent. Wrappers are a convenient way to modify an existing environment without having to alter the underlying code directly.<br><br>\n",
    "We modify the observation space slightly to encode the states as indicator vector ($v\\in[0,1]^{16}$), previously it was by integers in $\\{0,\\dots,15\\}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0674ba5-456a-410a-a657-3ea2f1576c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "       super(MyWrapper, self).__init__(env)\n",
    "       self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n,), dtype=np.float32) \n",
    "          #Our observations/states will be a vector of 16 reals in [0,1]\n",
    "\n",
    "    def observation(self, observation): #the state vector contains a 1 at index describing the former state\n",
    "        r = np.copy(self.observation_space.low)\n",
    "        r[observation] = 1.0\n",
    "        return r\n",
    "    \n",
    "env = MyWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4286907-544f-49e1-8316-5939981134b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b82d2c4-99dc-4415-80ab-650b59195300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "0.0\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "0.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.render())\n",
    "next_state, reward, episode_is_done, _ , _= env.step(2)\n",
    "print(env.render())\n",
    "print(reward)\n",
    "print(next_state)\n",
    "next_state, reward, episode_is_done, _ , _= env.step(1)\n",
    "print(env.render())\n",
    "print(reward)\n",
    "print(episode_is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccab2c-5b8c-4cb5-9b9d-3dcad4e94057",
   "metadata": {},
   "source": [
    "Now, we define a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6ef9a62-d796-4768-bfdc-8c0a8c1b8cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0] # 16\n",
    "n_actions = env.action_space.n  # 4\n",
    "HIDDEN_SIZE = 128 #128\n",
    "\n",
    "net= nn.Sequential(\n",
    "            nn.Linear(obs_size, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            #nn.Sigmoid(), \n",
    "            nn.Linear(HIDDEN_SIZE, n_actions)\n",
    "        )\n",
    "\n",
    "objective = nn.CrossEntropyLoss() #quite standard for classification tasks\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d196a7e-00f5-4c50-bcd9-eba99ad69d0f",
   "metadata": {},
   "source": [
    " We take the 16 inputs, feed them to a hidden layer with HIDDEN_SIZE nodes, ReLU activation function; there will be four outputs (the actions). The learning rate is set to lr=0.001. \n",
    "\n",
    "The ReLU activation function is defined as follows:\n",
    "$$ReLU(x)=\\left\\{\\begin{array}{lll}x &:& x\\geq 0,\\\\ 0 &:& x<0.\\end{array}\\right.$$\n",
    " \n",
    "As loss function we use the CrossEntropyLoss function, which is defined:\n",
    "$$ \\ell(x,c) = -x_c + \\ln\\left(\\sum_j \\exp(x_j)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655022f2-656c-47eb-992b-0cffcab3946a",
   "metadata": {},
   "source": [
    "Example: Consider $x=(3.2, 1.3,0.2, 0.8)^T$ and $c=0.$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a573336-d7e2-4ee9-8c08-9bb10a997950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2000, 1.3000, 0.2000, 0.8000]])\n",
      "tensor(0.2547)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "output = torch.tensor([[3.2, 1.3,0.2, 0.8]],dtype=torch.float)\n",
    "#softmax 0.7751, 0.1159, 0.386, 0.0703\n",
    "print(output)\n",
    "target = torch.tensor([0], dtype=torch.long)\n",
    "print(criterion(output, target))\n",
    "#print(-output[0][0]+np.log(np.exp(output[0][0])+np.exp(output[0][1])+np.exp(output[0][2])+np.exp(output[0][3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fce927-28b7-429c-8bf6-da1f8266b079",
   "metadata": {},
   "source": [
    "$\\ell(x,c)=0$ would be perfect.<br> ![title](CrossEntropyBinaryLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a9c0a-fd40-46a9-ac4c-d47ef0f32134",
   "metadata": {},
   "source": [
    "We now want to change our environment so that the agent is performing the action predicted by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb4fe54-608f-4288-a3c0-d3ffd1ea4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = nn.Softmax(dim=1) #Softmax converts the 4-dimensional output vector to a probability distribution\n",
    "\n",
    "def select_action(state):\n",
    "        state_t = torch.FloatTensor([state])\n",
    "        act_probs_t = sm(net(state_t))\n",
    "        act_probs = act_probs_t.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) #chooses randomly one of the 4 actions according to the probabilities returned by the net\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "248566d2-6cf6-4685-a848-a613b5185430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[ 0.0877,  0.0651, -0.0320,  0.1135]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2570, 0.2513, 0.2280, 0.2637]], grad_fn=<SoftmaxBackward0>)\n",
      "[ 0.08774319  0.06511313 -0.03198892  0.11354571]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here some explantion for the code above:\n",
    "a=[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "st=torch.FloatTensor([a])\n",
    "print(a)\n",
    "print(st)\n",
    "st2=net(st)\n",
    "st3=sm(net(st))\n",
    "\n",
    "print(st2)\n",
    "print(st3)\n",
    "print(st2.data.numpy()[0])\n",
    "\n",
    "np.random.choice(4,p=[0.7, 0.1, 0.1, 0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ff8f4d5-f9a3-4880-b599-c0ceb8f47be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 #100\n",
    "\n",
    "GAMMA = 0.9\n",
    "\n",
    "PERCENTILE = 30 #30\n",
    "REWARD_GOAL = 0.8\n",
    "\n",
    "from collections import namedtuple  #more readable tuples\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20957ab-bc73-453b-aba4-7f579fcf974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 3]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "b=a.copy()\n",
    "b[0]=4\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64cb4f7c-b6ff-4a05-8416-44b945c9c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAlpers\\AppData\\Local\\Temp\\ipykernel_12024\\1055420172.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  state_t = torch.FloatTensor([state])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.397, reward_mean=0.010\n",
      "1: loss=1.389, reward_mean=0.030\n",
      "2: loss=1.380, reward_mean=0.010\n",
      "3: loss=1.377, reward_mean=0.010\n",
      "4: loss=1.369, reward_mean=0.010\n",
      "5: loss=1.381, reward_mean=0.020\n",
      "6: loss=1.357, reward_mean=0.030\n",
      "7: loss=1.365, reward_mean=0.010\n",
      "8: loss=1.358, reward_mean=0.020\n",
      "9: loss=1.360, reward_mean=0.020\n",
      "10: loss=1.323, reward_mean=0.040\n",
      "11: loss=1.352, reward_mean=0.020\n",
      "12: loss=1.325, reward_mean=0.010\n",
      "13: loss=1.355, reward_mean=0.010\n",
      "14: loss=1.291, reward_mean=0.020\n",
      "15: loss=1.327, reward_mean=0.040\n",
      "16: loss=1.313, reward_mean=0.010\n",
      "17: loss=1.340, reward_mean=0.020\n",
      "18: loss=1.394, reward_mean=0.010\n",
      "19: loss=1.340, reward_mean=0.040\n",
      "20: loss=1.257, reward_mean=0.040\n",
      "21: loss=1.267, reward_mean=0.030\n",
      "22: loss=1.277, reward_mean=0.040\n",
      "23: loss=1.169, reward_mean=0.010\n",
      "24: loss=1.281, reward_mean=0.060\n",
      "25: loss=1.292, reward_mean=0.050\n",
      "26: loss=1.302, reward_mean=0.040\n",
      "27: loss=1.335, reward_mean=0.010\n",
      "28: loss=1.301, reward_mean=0.030\n",
      "29: loss=1.249, reward_mean=0.030\n",
      "30: loss=1.302, reward_mean=0.050\n",
      "31: loss=1.235, reward_mean=0.040\n",
      "32: loss=1.264, reward_mean=0.050\n",
      "33: loss=1.229, reward_mean=0.070\n",
      "34: loss=1.293, reward_mean=0.090\n",
      "35: loss=1.256, reward_mean=0.040\n",
      "36: loss=1.255, reward_mean=0.050\n",
      "37: loss=1.228, reward_mean=0.060\n",
      "38: loss=1.209, reward_mean=0.060\n",
      "39: loss=1.173, reward_mean=0.050\n",
      "40: loss=1.187, reward_mean=0.050\n",
      "41: loss=1.218, reward_mean=0.030\n",
      "42: loss=1.165, reward_mean=0.110\n",
      "43: loss=1.193, reward_mean=0.070\n",
      "44: loss=1.132, reward_mean=0.070\n",
      "45: loss=1.225, reward_mean=0.130\n",
      "46: loss=1.151, reward_mean=0.110\n",
      "47: loss=1.198, reward_mean=0.060\n",
      "48: loss=1.184, reward_mean=0.190\n",
      "49: loss=1.153, reward_mean=0.110\n",
      "50: loss=1.224, reward_mean=0.080\n",
      "51: loss=1.159, reward_mean=0.090\n",
      "52: loss=1.253, reward_mean=0.100\n",
      "53: loss=1.216, reward_mean=0.090\n",
      "54: loss=1.175, reward_mean=0.100\n",
      "55: loss=1.153, reward_mean=0.200\n",
      "56: loss=1.160, reward_mean=0.140\n",
      "57: loss=1.127, reward_mean=0.160\n",
      "58: loss=1.185, reward_mean=0.110\n",
      "59: loss=1.132, reward_mean=0.140\n",
      "60: loss=1.130, reward_mean=0.160\n",
      "61: loss=1.130, reward_mean=0.130\n",
      "62: loss=1.143, reward_mean=0.090\n",
      "63: loss=1.146, reward_mean=0.160\n",
      "64: loss=1.188, reward_mean=0.110\n",
      "65: loss=1.087, reward_mean=0.140\n",
      "66: loss=1.073, reward_mean=0.150\n",
      "67: loss=1.061, reward_mean=0.150\n",
      "68: loss=1.135, reward_mean=0.170\n",
      "69: loss=1.149, reward_mean=0.080\n",
      "70: loss=1.114, reward_mean=0.150\n",
      "71: loss=1.099, reward_mean=0.220\n",
      "72: loss=1.052, reward_mean=0.240\n",
      "73: loss=1.116, reward_mean=0.160\n",
      "74: loss=1.062, reward_mean=0.210\n",
      "75: loss=1.014, reward_mean=0.230\n",
      "76: loss=1.022, reward_mean=0.150\n",
      "77: loss=1.022, reward_mean=0.240\n",
      "78: loss=1.098, reward_mean=0.220\n",
      "79: loss=1.043, reward_mean=0.260\n",
      "80: loss=1.043, reward_mean=0.160\n",
      "81: loss=1.038, reward_mean=0.220\n",
      "82: loss=0.972, reward_mean=0.240\n",
      "83: loss=1.065, reward_mean=0.210\n",
      "84: loss=0.980, reward_mean=0.150\n",
      "85: loss=1.015, reward_mean=0.340\n",
      "86: loss=1.026, reward_mean=0.190\n",
      "87: loss=0.970, reward_mean=0.250\n",
      "88: loss=0.995, reward_mean=0.250\n",
      "89: loss=1.108, reward_mean=0.230\n",
      "90: loss=0.976, reward_mean=0.240\n",
      "91: loss=0.946, reward_mean=0.200\n",
      "92: loss=0.972, reward_mean=0.250\n",
      "93: loss=0.989, reward_mean=0.300\n",
      "94: loss=0.983, reward_mean=0.300\n",
      "95: loss=0.938, reward_mean=0.350\n",
      "96: loss=0.923, reward_mean=0.260\n",
      "97: loss=0.917, reward_mean=0.330\n",
      "98: loss=0.963, reward_mean=0.400\n",
      "99: loss=0.904, reward_mean=0.330\n",
      "100: loss=0.889, reward_mean=0.260\n",
      "101: loss=0.909, reward_mean=0.380\n",
      "102: loss=0.928, reward_mean=0.440\n",
      "103: loss=0.940, reward_mean=0.400\n",
      "104: loss=0.915, reward_mean=0.410\n",
      "105: loss=0.898, reward_mean=0.380\n",
      "106: loss=0.932, reward_mean=0.460\n",
      "107: loss=0.955, reward_mean=0.310\n",
      "108: loss=0.876, reward_mean=0.360\n",
      "109: loss=0.878, reward_mean=0.340\n",
      "110: loss=0.863, reward_mean=0.410\n",
      "111: loss=0.887, reward_mean=0.410\n",
      "112: loss=0.937, reward_mean=0.360\n",
      "113: loss=0.930, reward_mean=0.400\n",
      "114: loss=0.882, reward_mean=0.440\n",
      "115: loss=0.939, reward_mean=0.410\n",
      "116: loss=0.879, reward_mean=0.410\n",
      "117: loss=0.866, reward_mean=0.460\n",
      "118: loss=0.853, reward_mean=0.450\n",
      "119: loss=0.856, reward_mean=0.440\n",
      "120: loss=0.861, reward_mean=0.430\n",
      "121: loss=0.791, reward_mean=0.460\n",
      "122: loss=0.866, reward_mean=0.400\n",
      "123: loss=0.878, reward_mean=0.520\n",
      "124: loss=0.863, reward_mean=0.400\n",
      "125: loss=0.808, reward_mean=0.540\n",
      "126: loss=0.813, reward_mean=0.480\n",
      "127: loss=0.771, reward_mean=0.390\n",
      "128: loss=0.851, reward_mean=0.580\n",
      "129: loss=0.810, reward_mean=0.520\n",
      "130: loss=0.798, reward_mean=0.460\n",
      "131: loss=0.841, reward_mean=0.500\n",
      "132: loss=0.806, reward_mean=0.560\n",
      "133: loss=0.779, reward_mean=0.510\n",
      "134: loss=0.787, reward_mean=0.620\n",
      "135: loss=0.818, reward_mean=0.610\n",
      "136: loss=0.783, reward_mean=0.570\n",
      "137: loss=0.778, reward_mean=0.500\n",
      "138: loss=0.786, reward_mean=0.540\n",
      "139: loss=0.864, reward_mean=0.600\n",
      "140: loss=0.771, reward_mean=0.570\n",
      "141: loss=0.790, reward_mean=0.550\n",
      "142: loss=0.789, reward_mean=0.690\n",
      "143: loss=0.763, reward_mean=0.690\n",
      "144: loss=0.812, reward_mean=0.550\n",
      "145: loss=0.769, reward_mean=0.600\n",
      "146: loss=0.734, reward_mean=0.730\n",
      "147: loss=0.762, reward_mean=0.630\n",
      "148: loss=0.823, reward_mean=0.640\n",
      "149: loss=0.795, reward_mean=0.670\n",
      "150: loss=0.752, reward_mean=0.750\n",
      "151: loss=0.757, reward_mean=0.630\n",
      "152: loss=0.751, reward_mean=0.730\n",
      "153: loss=0.716, reward_mean=0.650\n",
      "154: loss=0.736, reward_mean=0.550\n",
      "155: loss=0.740, reward_mean=0.650\n",
      "156: loss=0.741, reward_mean=0.660\n",
      "157: loss=0.740, reward_mean=0.720\n",
      "158: loss=0.734, reward_mean=0.690\n",
      "159: loss=0.682, reward_mean=0.670\n",
      "160: loss=0.747, reward_mean=0.690\n",
      "161: loss=0.711, reward_mean=0.680\n",
      "162: loss=0.726, reward_mean=0.740\n",
      "163: loss=0.728, reward_mean=0.660\n",
      "164: loss=0.664, reward_mean=0.630\n",
      "165: loss=0.702, reward_mean=0.670\n",
      "166: loss=0.687, reward_mean=0.670\n",
      "167: loss=0.704, reward_mean=0.720\n",
      "168: loss=0.681, reward_mean=0.760\n",
      "169: loss=0.680, reward_mean=0.690\n",
      "170: loss=0.655, reward_mean=0.720\n",
      "171: loss=0.624, reward_mean=0.720\n",
      "172: loss=0.639, reward_mean=0.710\n",
      "173: loss=0.666, reward_mean=0.790\n",
      "174: loss=0.669, reward_mean=0.730\n",
      "175: loss=0.592, reward_mean=0.810\n",
      "--- 84.17484593391418 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "iter_no = 0\n",
    "reward_mean = 0\n",
    "full_batch = []\n",
    "batch = []\n",
    "episode_steps = []\n",
    "episode_reward = 0.0\n",
    "state,_ = env.reset() \n",
    "    \n",
    "while reward_mean < REWARD_GOAL:\n",
    "        action = select_action(state)\n",
    "        next_state, reward, episode_is_done, _ , _= env.step(action)\n",
    "\n",
    "        episode_steps.append(EpisodeStep(observation=state, action=action))\n",
    "        episode_reward += reward\n",
    "        \n",
    "        #print(episode_steps)\n",
    "        \n",
    "        if episode_is_done: # Episode finished      \n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            \n",
    "            \n",
    "            #print(len(batch))\n",
    "            \n",
    "            next_state,_ = env.reset()\n",
    "            episode_steps = []\n",
    "            episode_reward = 0.0\n",
    "             \n",
    "            if len(batch) == BATCH_SIZE: # New set of batches ready --> select \"elite\"\n",
    "                \n",
    "                #print(\"Batch full\")\n",
    "                #print(batch)\n",
    "                #print(\"\\n\")\n",
    "                #input(\"Press Enter to continue...\") \n",
    "\n",
    "                \n",
    "                reward_mean = float(np.mean(list(map(lambda s: s.reward, batch)))) #compute mean reward (lambda is inline function)\n",
    "                elite_candidates= batch\n",
    "                #elite_candidates= batch + full_batch\n",
    "                returnG = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), elite_candidates))\n",
    "                reward_bound = np.percentile(returnG, PERCENTILE) #lowest score that is greater than PERCENTILE% of scores in the data set\n",
    "                                                                  #Keep the highest 100-PERCENTILE %\n",
    "                #print(\"Batch finished\", returnG, reward_bound)\n",
    "                #input(\"Press Enter to continue...\") \n",
    "                \n",
    "                train_obs = []\n",
    "                train_act = []\n",
    "                elite_batch = []\n",
    "                \n",
    "                for example, discounted_reward in zip(elite_candidates, returnG):\n",
    "                        if discounted_reward > reward_bound:\n",
    "                        #if discounted_reward >= reward_bound:    \n",
    "                              train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "                              train_act.extend(map(lambda step: step.action, example.steps))\n",
    "                              elite_batch.append(example)\n",
    "                full_batch=elite_batch\n",
    "                state=train_obs\n",
    "                acts=train_act\n",
    "                \n",
    "                #print(state)\n",
    "                #print(acts)\n",
    "                #input(\"Press Enter to continue...\")  \n",
    "                \n",
    "                #Do the training\n",
    "                if len(full_batch) != 0 : # just in case empty during an iteration\n",
    "                  state_t = torch.FloatTensor(state) #batch of states: [[1.0,0,0,0,0,0,0,0,0,0],[1,...]]               \n",
    "                  acts_t = torch.LongTensor(acts) # batch of actions: [0,2,3,1,..]               \n",
    "                               \n",
    "                  #print(state_t)\n",
    "                  #print(acts_t)\n",
    "                  #input(\"Press Enter to continue...\")  \n",
    "                  optimizer.zero_grad() #it is good practice to do this, initializing the gradient computations\n",
    "                  action_scores_t = net(state_t)\n",
    "                  \n",
    "                  #print(action_scores_t)\n",
    "                  #input(\"Press Enter to continue...\") \n",
    "                \n",
    "                  loss_t = objective(action_scores_t, acts_t)\n",
    "                  loss_t.backward() #computes the gradients\n",
    "                  optimizer.step() #updates the weights according to the gradients\n",
    "                  print(\"%d: loss=%.3f, reward_mean=%.3f\" % (iter_no, loss_t.item(), reward_mean))\n",
    "                  iter_no += 1\n",
    "                batch = [] #empty the batch\n",
    "        state = next_state\n",
    "        \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461213f-86e0-4c53-82b9-f32e70270107",
   "metadata": {},
   "source": [
    "Let's test (explicitely) what our NN learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f2af400-f83b-43e1-a401-8a8649f05236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " {'prob': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e3266e1-52f2-4307-9171-754af1a83b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Step: 1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Step: 2\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "Step: 3\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "Step: 4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "\n",
      "Step: 5\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "Step: 6\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "done=0\n",
    "counter=0\n",
    "MAXCOUNTER=15\n",
    "\n",
    "print(\"Step: \"+str(counter))\n",
    "print(env.render())\n",
    "\n",
    "while done !=1 and counter<MAXCOUNTER:\n",
    "  state_t = torch.FloatTensor([state])\n",
    "  action_scores_t = net(state_t)\n",
    "  act_probs_t = sm(action_scores_t)\n",
    "  #print(act_probs_t)\n",
    "  act_probs = act_probs_t.data.numpy()[0]\n",
    "  proposed_action=np.argmax(act_probs)\n",
    "  next_state, reward, done, _ , _= env.step(proposed_action)\n",
    "  state=next_state.tolist() #changes NumPy array to a Python list\n",
    "  counter+=1\n",
    "  print(\"Step: \"+str(counter))\n",
    "  print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf8946-dad5-41dd-9463-c78d0c42aab4",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "__Task__: Make adaptations to the above code (i.e. use the cross-entropy method from deep inforcement learning) to find the so-called Longest Increasing Subsequence (LIS) of the sequence [17, 15, 3, 22, 9, 13, 33, 21, 50, 40, 50, 42, 65, 60, 5, 70]. <br>\n",
    "\n",
    "The Longest Increasing Subsequence Problem asks to find a subsequence of a given sequence in which the subsequence's elements are sorted in an ascending order and in which the subsequence is as long as possible. For instance, the LIS of the sequence [0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15] is [0, 2, 6, 9, 11, 15].\n",
    "\n",
    "__Hints__: The following are just hints. You don't need to follow them as long as you find a rather long increasing subsequence with your code. Aim for finding the longest increasing subsequence.<br>\n",
    "\n",
    "* You have to think about how you want to represent the states and actions. (For instance, you can represent the current selection of the subsequence by a binary 0/1 vector, where a 1 in position i represents that you have currently chosen the ith term in the sequence; an action could be a binary vector with a single 1 representing the term which you want to add to the subsequence.) \n",
    "\n",
    "* You probably need to program three mini-functions: One that gives you an initial state, one that selects an action as recommened by your NN, and one that performs a specified action given a state.\n",
    "\n",
    "* Don't use this line \"while reward_mean < REWARD_GOAL:\". Substitute it in an appropriate way so that you find the longest (or at least a rather long) increasing subsequence of the given input sequence.\n",
    "\n",
    "* An episode could create an increasing subsequence. Then the corresponding reward could be the length of this increasing subsequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc88588-a924-49c4-94aa-d52db87ab5d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f082c02e-2097-4de6-bc9c-e598761d3d05",
   "metadata": {},
   "source": [
    "This Jupyter Notebook is inspired by https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained and https://gsverhoeven.github.io/post/frozenlake-qlearning-convergence/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
